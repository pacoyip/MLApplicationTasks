1.Description of what your team did
  Add PCA process in the former milestones.

2.Methods used to accomplish each part
    1)Two PCA processes:
      we do dimensionality reduction with 2 different PCA: PCA with randomized SVD, and Kernel PCA with RBF kernel.
      and by using 10-fold cross validation, we find that after dimensionality reduction, the 5 methods
      in milestone_1 and milestone_2 all get worse.
      
    2)Correlation Heatmap and main feature analysis
      To support our analysis in the part6 of this report, we generate a correlation plot of
      the features to see how related one feature is to the next, and we pick the two mainly correlated
      features to put in the 6 methods.

3.Potential difficulties faced
  we are not sure why the kernel PCA are worse than normal PCA; and why in the Gaussian Process with RBF kernel,
  the kernel PCA is better than normal PCA.

4.Resources used
  1) Data from Kaggle project(Titanic: Machine Learning from Disaster)
  2) Get PCA Process, Gaussian Process Classifier, kernel and cross validation from sk-learn package

5.Description of how to run the code in the folder
  Use any python interpreter to “milestones3.py”, change pd.read_csv (About Line 42) path
  to the path of data “train.csv”; Run the code.

6.Analysis:
  1).  After PCA with randomized SVD and kernel PCA, the accuracy of all 5 methods are getting worse.
       We think the main reason that lead to this result is because of our data set only contains 6 features, and
       the number of data points is only 891, which is too small. After PCA, our data set loss too many information,
       and the dimension reduction will not compensate to the loss, which contributes to a worse performance.
       
  2).  Compare features generated by PCA with the two highest correlated features
       We set the dimension of features after PCA is 2, So ,we pick the two highest correlated features (we call them
       two main features in the later part) from the preprocessed data set to compare with features generated by PCA.
       We find that the two main features will lead the accuracy of 5 methods close to use all the 6 features
       in the 5 methods, and also perform better than the 2 features generated by PCA with SVD, and the 2 features
       generated by kernel PCA.
          The reason why this result occur we think is because of the unique properties of our data set: our data set
       is about Titanic, we know in this famous tragedy, female and people in the VIP class are more easy to
       survive.Therefore, this two features have huge impact to the target data y: survived or died.
       Before PCA, both the dataset with 6 features and the dataset two main features have a relatively good accuracy. 
       After PCA, the features generated are not that related with the target data y, so,
       the accuracy is worse.
      
  3).  Compare two different PCAs:
       In most case, we find that the result of PCA with SVD is better than PCA with RBF kernel, but only in the
       Gaussian Process with RBF kernel, PCA with RBF kernel is better than PCA with SVD.
       The reason that why in most case PCA with SVD is better than PCA with RBF kernel is because SVD let PCA more
       stable, so, with only two features, PCA with SVD performs better.
       The reason why in GP with RBF kernel, PCA with RBF kernel is better than PCA with SVD we think might be:
       RBF is one of Similarity-Based methods, this may lead to PCA with RBF kernel performs better in Gaussian
       process with RBF kernel.
       
  4)Above all, we think our data set should not use PCA, no matter is with SVD or with Kernel.


7.Result of each method:
  1)Process without PCA:
    Negative-log-loss of logreg after 10fold-val: 7.29 (+/- 2.01)
    Negative-log-loss of percep after 10fold-val: 8.96 (+/- 3.48)
    Negative-log-loss of DCtree after 10fold-val: 6.66 (+/- 3.35)
    Negative-log-loss of GP+RBF after 10fold-val: 6.82 (+/- 1.52)
    Negative-log-loss of GP+Cst after 10fold-val: 13.26 (+/- 0.20)
  2)Process with only two main features:
    Negative-log-loss of logreg of two main features after 10fold-val: 7.37 (+/- 1.93)
    Negative-log-loss of percep of two main features after 10fold-val: 9.03 (+/- 2.86)
    Negative-log-loss of DCtree of two main features after 10fold-val: 7.83 (+/- 2.19)
    Negative-log-loss of GP+RBF of two main features after 10fold-val: 7.83 (+/- 2.19)
    Negative-log-loss of GP+Cst of two main features after 10fold-val: 13.26 (+/- 0.20)
  3)Process with PCA + randomized SVD:
    Negative-log-loss of logreg after PCA & 10fold-val: 10.69 (+/- 4.00)
    Negative-log-loss of percep after PCA & 10fold-val: 12.01 (+/- 4.17)
    Negative-log-loss of DCtree after PCA & 10fold-val: 7.01 (+/- 3.24)
    Negative-log-loss of GP+RBF after PCA & 10fold-val: 8.99 (+/- 5.54)
    Negative-log-loss of GP+Cst after PCA & 10fold-val: 13.26 (+/- 0.20)
  4)Process with kernel PCA
    Negative-log-loss of logreg after KPCA & 10fold-val: 10.77 (+/- 5.02)
    Negative-log-loss of percep after KPCA & 10fold-val: 12.44 (+/- 1.90)
    Negative-log-loss of DCtree after KPCA & 10fold-val: 7.60 (+/- 2.72)
    Negative-log-loss of GP+RBF after KPCA & 10fold-val: 7.21 (+/- 3.13)
    Negative-log-loss of GP+Cst after KPCA & 10fold-val: 13.26 (+/- 0.20)


